# Energy Consumption Clustering Pipeline

Un pipeline completo de anÃ¡lisis y clustering de datos de consumo energÃ©tico con interfaz web integrada. Este proyecto implementa mÃºltiples algoritmos de agrupamiento (K-Means, Bisecting K-Means, Spectral Clustering) y tÃ©cnicas de procesamiento de datos para identificar patrones de consumo en viviendas.

## ğŸš€ CaracterÃ­sticas Principales

- **Pipeline modular** con notebooks independientes para cada fase del anÃ¡lisis
- **Interfaz web** desarrollada en Flask para uso sin conocimientos tÃ©cnicos
- **MÃºltiples algoritmos de clustering** con optimizaciÃ³n automÃ¡tica de hiperparÃ¡metros
- **GeneraciÃ³n automÃ¡tica de mÃ¡s de 40 mÃ©tricas** estadÃ­sticas y espectrales
- **Visualizaciones interactivas** en 2D y 3D con PCA
- **Ãrboles de decisiÃ³n** para interpretabilidad de clusters
- **Arquitectura extensible** para nuevos algoritmos y datasets

## ğŸ“‹ Requisitos

- **Python 3.9+**
- Entorno virtual recomendado (`venv`, `conda` o similar)

## ğŸ› ï¸ InstalaciÃ³n

1. Clona el repositorio:
```bash
git clone [URL_DEL_REPOSITORIO]
cd [NOMBRE_DEL_PROYECTO]
```

2. Instala las dependencias:
```bash
pip install -r requirements.txt
```

Esto instalarÃ¡ automÃ¡ticamente todas las librerÃ­as necesarias: pandas, scikit-learn, matplotlib, fancyimpute, Flask, entre otras.

## ğŸ“ Estructura del Proyecto

```
â”œâ”€â”€ Algorithm/                          # NÃºcleo del proyecto
â”‚   â”œâ”€â”€ dataset/                       # Conjuntos de datos intermedios
â”‚   â”œâ”€â”€ img_results/                   # GrÃ¡ficos finales
â”‚   â”œâ”€â”€ flowchart/                     # Diagramas de flujo
â”‚   â”œâ”€â”€ pkls/                          # Modelos serializados
â”‚   â”œâ”€â”€ data_analysis_results/         # Reportes de anÃ¡lisis
â”‚   â””â”€â”€ [notebooks 1-8].ipynb         # Pipeline principal
â”œâ”€â”€ data_raw/                          # Datos originales (Datadis)
â”œâ”€â”€ interfaz/                          # Interfaz web Flask
â”‚   â”œâ”€â”€ src/                          # MÃ³dulos de procesamiento
â”‚   â””â”€â”€ app.py                        # AplicaciÃ³n principal
â”œâ”€â”€ DEPRECATED_USE_*/                  # Exploraciones anteriores
â””â”€â”€ README.md
```

## ğŸ”„ Pipeline de EjecuciÃ³n

### OpciÃ³n 1: Notebooks (Uso TÃ©cnico)

Ejecuta los notebooks en el siguiente orden:

1. **`1_concatenate_csv_and_report.ipynb`**
   - Unifica archivos CSV individuales
   - Genera dataset consolidado y reporte inicial

2. **`2_EDA_and_preproccesed.ipynb`**
   - AnÃ¡lisis exploratorio de datos (EDA)
   - Tratamiento de valores atÃ­picos y nulos

3. **`3_generate_features.ipynb`**
   - Calcula +40 mÃ©tricas estadÃ­sticas y espectrales
   - Genera `dataset/generated_features.csv`

4. **`4_impute_new_data.ipynb`**
   - ImputaciÃ³n con 3 estrategias:
     - KNNImputer (vecinos cercanos)
     - SoftImpute (descomposiciÃ³n matricial)
     - Drop (eliminaciÃ³n conservadora)

5. **`5_scaler.ipynb`**
   - EstandarizaciÃ³n con StandardScaler/MinMaxScaler

6. **`6_1_Select_K_Kmeans.ipynb`** / **`6_2_Select_K_BKmeans.ipynb`** / **`6_3_Select_K_Spectral_Clustering.ipynb`**
   - SelecciÃ³n Ã³ptima del nÃºmero de clusters
   - Entrenamiento y serializaciÃ³n de modelos

7. **`7_1_visualization_kmeans_and_bkmeans.ipynb`** / **`7_2_visualization_spectral_clustering.ipynb`**
   - Visualizaciones 2D/3D con PCA
   - GrÃ¡ficas de clusters vs features

8. **`8_binary_tree.ipynb`**
   - Ãrbol de decisiÃ³n para interpretabilidad
   - Reglas de asignaciÃ³n a clusters

### OpciÃ³n 2: Interfaz Web (Uso Simplificado)

Para usuarios sin conocimientos tÃ©cnicos:

```bash
cd interfaz
python app.py
```

Accede a `http://127.0.0.1:5000/` y sigue el flujo guiado:

1. **Carga de Datos** - Selecciona carpeta con CSVs
2. **GeneraciÃ³n e ImputaciÃ³n** - CÃ¡lculo automÃ¡tico de caracterÃ­sticas
3. **Escalado y Clustering** - SelecciÃ³n de algoritmo y optimizaciÃ³n
4. **VisualizaciÃ³n** - GrÃ¡ficos automÃ¡ticos de resultados
5. **Interpretabilidad** - Ãrbol de decisiÃ³n opcional

## ğŸ§® Algoritmos Implementados

### Clustering
- **K-Means**: Clustering particional clÃ¡sico
- **Bisecting K-Means**: Variante jerÃ¡rquica divisiva
- **Spectral Clustering**: Basado en teorÃ­a de grafos

### SelecciÃ³n de K
- **MÃ©todo K-ISAC_TLP**: OptimizaciÃ³n automÃ¡tica de hiperparÃ¡metros
- **MÃ©tricas de validaciÃ³n**: Silhouette, Davies-Bouldin Index, MAE

### ImputaciÃ³n
- **KNN Imputer**: Basado en k-vecinos mÃ¡s cercanos
- **Soft Impute**: DescomposiciÃ³n matricial
- **Drop**: EliminaciÃ³n conservadora


## ğŸ¯ Casos de Uso

- **Empresas energÃ©ticas**: SegmentaciÃ³n de clientes y tarifas personalizadas
- **InvestigaciÃ³n acadÃ©mica**: AnÃ¡lisis de patrones de consumo
- **PolÃ­ticas pÃºblicas**: IdentificaciÃ³n de grupos vulnerables
- **Eficiencia energÃ©tica**: DetecciÃ³n de consumos anÃ³malos

## ğŸ”§ MÃ³dulos de la Interfaz

Los scripts en `interfaz/src/` encapsulan la lÃ³gica de cada fase:

- `EDA.py` - AnÃ¡lisis exploratorio
- `generate_features.py` - CÃ¡lculo de caracterÃ­sticas
- `impute_data.py` - ImputaciÃ³n de datos
- `scaler.py` - Escalado de variables
- `select_k.py` - OptimizaciÃ³n de clusters
- `visualization.py` - GeneraciÃ³n de grÃ¡ficos
- `binary_tree.py` - Interpretabilidad

## ğŸ“ˆ Visualizaciones

- **PCA 2D/3D**: ReducciÃ³n dimensional para visualizaciÃ³n
- **Centroides**: Perfiles promedio de cada cluster
- **DistribuciÃ³n**: Histogramas y boxplots por grupo
- **Mapas de calor**: Correlaciones entre variables
- **Ãrboles de decisiÃ³n**: Reglas interpretables

## ğŸ“ Formato de Datos

Los datos de entrada deben seguir el formato Datadis:

- Las columnas deben ser: `cups`, `fecha`, `hora`, `consumo_kWh`, `metodoObtencion`, `energiaVertida_kWh`.  

**Ejemplo de una fila de datos:**  
`0d9378df292f;2021/08/07;01:00;0,000;;`  

## ğŸ”¬ Extensibilidad

El diseÃ±o modular permite fÃ¡cilmente:

- **Nuevos algoritmos**: AÃ±adir DBSCAN, GMM, etc.
- **Datasets diferentes**: Adaptar a nuevas ciudades/perÃ­odos
- **MÃ©tricas personalizadas**: Incorporar features especÃ­ficas
- **Visualizaciones**: Crear nuevos tipos de grÃ¡ficos


---

**Nota**: Este pipeline ha sido desarrollado como herramienta de investigaciÃ³n siguiendo buenas prÃ¡cticas en ciencia de datos. La arquitectura modular facilita su adaptaciÃ³n a diferentes contextos y la incorporaciÃ³n de nuevas tÃ©cnicas de anÃ¡lisis.